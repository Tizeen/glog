<!DOCTYPE html>
<html lang="zh-cn"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   <meta name="description" content="GPU 作为目前图形处理和 AI 任务必须的资源，这里简单记录一下在 K8S 中的程序，它们是如何使用 GPU 的。
这里以 AWS EKS 进行相关记录

  
    确保使用的机器是有 GPU 硬件，例如 AWS 的 g4dn，g6 等机型就是带有 GPU 硬件的
  
  
    在 EKS 中添加 GPU Node Group，注意使用的 AMI （操作系统镜像）需要带有 GPU 驱动和 cuda，选择 Amazon Linux 2023 Nvidia 的镜像
  
  
    安装 k8s-device-plugin 组件，它会以 daemonset 的方式运行在节点上，如果我们的 GPU 节点配置了对应的污点，安装时需要修改对应的容忍机制
  
  
    安装完 k8s-device-plugin 之后，我们通过 kubectl describe node，可以在 Capacity 中可以看到可用的 GPU 数量。当我们登录 GPU 节点，执行 nvidia-smi ，也可以看到 GPU 的相关信息
  
  
    使用 GPU 感知的镜像来启动 Pod，例如 nvidia/cuda:11.0-base，它们内部已经包含了使用 GPU 需要的库、驱动程序、工具，使得容器内的应用程序能够有效地利用GPU资源进行计算在 deployment 中，我们需要在 requests 中标明需要的 GPU
  

至此，我们就可以在 k8s 的容器中调用节点的 GPU 资源了。">  

  <title>
    
      K8S中的服务使用GPU
    
  </title>


  <link rel="shortcut icon" type="image/x-icon" href="/" />
  
  
  
  <link rel="stylesheet" href="/css/main.900100e9dbee2d56c58fac8bb717037cae7e26a9c36c29d2ff587bdd65f0cbbe510b41d81a3bb234919cdfdc7550d786b2fab70c8fc507772d732fe097106d12.css" integrity="sha512-kAEA6dvuLVbFj6yLtxcDfK5&#43;JqnDbCnS/1h73WXwy75RC0HYGjuyNJGc39x1UNeGsvq3DI/FB3ctcy/glxBtEg==" />
  
</head>
<body a="auto">
        <main class="page-content" aria-label="Content">
            <div class="w">
<a href="/">..</a>


<article>
    <p class="post-meta">
        <time datetime="2024-10-12 10:55:36 &#43;0000 &#43;0000">
            2024-10-12
        </time>
    </p>

    <h1>K8S中的服务使用GPU</h1>

    

    <p>GPU 作为目前图形处理和 AI 任务必须的资源，这里简单记录一下在 K8S 中的程序，它们是如何使用 GPU 的。</p>
<p>这里以 AWS EKS 进行相关记录</p>
<ol class="wp-block-list">
  <li class="has-medium-font-size">
    确保使用的机器是有 GPU 硬件，例如 AWS 的 g4dn，g6 等机型就是带有 GPU 硬件的
  </li>
  <li>
    在 EKS 中添加 GPU Node Group，注意使用的 AMI （操作系统镜像）需要带有 GPU 驱动和 cuda，选择 Amazon Linux 2023 Nvidia 的镜像
  </li>
  <li>
    安装 <a href="https://github.com/NVIDIA/k8s-device-plugin"><strong>k8s-device-plugin</strong></a> 组件，它会以 daemonset 的方式运行在节点上，如果我们的 GPU 节点配置了对应的污点，安装时需要修改对应的容忍机制
  </li>
  <li>
    安装完 k8s-device-plugin 之后，我们通过 <code>kubectl describe node</code>，可以在 Capacity 中可以看到可用的 GPU 数量。当我们登录 GPU 节点，执行 <code>nvidia-smi</code> ，也可以看到 GPU 的相关信息<br /><img loading="lazy" decoding="async" width="350" height="125" class="wp-image-255" style="width: 350px;" src="https://glog.likungong.com/wp-content/uploads/2024/10/20241012-181352.jpeg" alt="" srcset="https://glog.likungong.com/wp-content/uploads/2024/10/20241012-181352.jpeg 848w, https://glog.likungong.com/wp-content/uploads/2024/10/20241012-181352-300x107.jpeg 300w, https://glog.likungong.com/wp-content/uploads/2024/10/20241012-181352-768x274.jpeg 768w" sizes="auto, (max-width: 350px) 100vw, 350px" />
  </li>
  <li>
    使用 GPU 感知的镜像来启动 Pod，例如 <code>nvidia/cuda:11.0-base</code>，它们内部已经包含了使用 GPU 需要的库、驱动程序、工具，使得容器内的应用程序能够有效地利用GPU资源进行计算<br />在 deployment 中，我们需要在 requests 中标明需要的 GPU<br /><img loading="lazy" decoding="async" width="350" height="103" class="wp-image-256" style="width: 350px;" src="https://glog.likungong.com/wp-content/uploads/2024/10/20241012-181532.jpeg" alt="" srcset="https://glog.likungong.com/wp-content/uploads/2024/10/20241012-181532.jpeg 786w, https://glog.likungong.com/wp-content/uploads/2024/10/20241012-181532-300x89.jpeg 300w, https://glog.likungong.com/wp-content/uploads/2024/10/20241012-181532-768x227.jpeg 768w" sizes="auto, (max-width: 350px) 100vw, 350px" />
  </li>
</ol>
<p>至此，我们就可以在 k8s 的容器中调用节点的 GPU 资源了。</p>

</article>

                

            </div>
        </main>
    </body>
</html>
